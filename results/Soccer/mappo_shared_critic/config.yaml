# MAPPO Soccer Configuration
# Algorithm: Multi-Agent Proximal Policy Optimization with Shared Critic
# Environment: Soccer (Unity ML-Agents)

# Environment Configuration
env_id: Soccer
device: cuda
max_steps: 1000000
seed: 1
zerosum: true
teams: [[0, 2], [1, 3]]

# Algorithm Configuration
algo: mappo
shared_policy: false # Individual policies per agent
shared_critic: true # Shared centralized critic

# Learning Parameters
actor_lr: 0.0005
critic_lr: 0.0005
entropy_coef: 0.01
gamma: 0.99
gae_lambda: 0.95
clip_param: 0.2
ppo_epoch: 5
num_mini_batch: 32
use_clipped_value_loss: true

# Network Architecture
hidden_sizes: [128, 128]
state_dependent_std: false
use_role_id: false

# Training Configuration
n_steps: 2048
use_reward_norm: true

# Gradient Clipping
use_max_grad_norm: true
max_grad_norm: 10.0

# Evaluation Configuration
eval_interval: 10000
competitive_eval_episodes: 5
max_model_snapshots: 10

# Logging and Monitoring
log_interval: 1000
use_wandb: false
render: false

# System Configuration
worker_id: 1
cuda: true
torch_threads: 1
cuda_deterministic: true

# Run Configuration
run_name: test_run
config: configs/env_tuned/mappo_soccer.yaml
